{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e6b918-afac-43e8-b338-43f8ac64014f",
   "metadata": {},
   "source": [
    "##  Deep Neural Networks Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e066d-2849-4a6e-9c71-bb98d605ae07",
   "metadata": {},
   "source": [
    "In this project, you will be working with a real-world data set from the Las Vegas Metropolitan Police Department. The dataset  contains information about the reported incidents, including the time and location of the crime, type of incident, and number of persons involved. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e87fac7-352a-4c39-b087-76254b5e2743",
   "metadata": {},
   "source": [
    "The dataset is downloaded from the public docket at: \n",
    "https://opendata-lvmpd.hub.arcgis.com\n",
    "\n",
    "let's read the csv file and transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5988beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\030742879\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\030742879\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637211a4-582f-426b-a127-c3f284463f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf40b02-80b6-4abc-a662-f7ed50a65181",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('../../datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ca1d15-3955-4971-a3c4-c1a73b62edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/LVMPD-Stats.csv', parse_dates=['ReportedOn'],\n",
    "                 usecols = ['X', 'Y', 'ReportedOn',\n",
    "                            'Area_Command','NIBRSOffenseCode',\n",
    "                            'VictimCount' ] )\n",
    "\n",
    "df['DayOfWeek'] = df['ReportedOn'].dt.day_name()\n",
    "df['Time' ]     = df['ReportedOn'].dt.hour\n",
    "df.drop(columns = 'ReportedOn', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ddc413d-ba3f-4204-bc18-7fdd4de8d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['X'] = df['X'] \n",
    "df['Y'] = df['Y'] \n",
    "df['Time'] = pd.factorize(df['Time'])[0]\n",
    "df['DayOfWeek'] = pd.factorize(df['DayOfWeek'])[0]\n",
    "df.Area_Command = pd.factorize(df['Area_Command'])[0]\n",
    "df.VictimCount = pd.factorize(df['VictimCount'])[0]\n",
    "df.NIBRSOffenseCode = pd.factorize(df['NIBRSOffenseCode'])[0]\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c6162f-9686-4195-818d-950a6368c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['X', 'Y', 'Area_Command', 'NIBRSOffenseCode',\n",
    "       'DayOfWeek', 'Time','VictimCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90bc78a-6d1b-4fe4-a1b0-8333aec1c851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275, 7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651605b1-8d2c-4d3e-a09e-9aef6e550fc6",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal is to build a predictive model that is trained on the following data:\n",
    "* latitude and longitude (location)\n",
    "* Hour of the day\n",
    "* Day of the week\n",
    "* Area-of-command code: The police designation of the bureau of the operation.\n",
    "* Classification code for the crime committed\n",
    "  \n",
    "The predicted variable is the number of persons involved in the accident.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54f0b8-83f9-4db9-88f9-f5a595342069",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "* print a few rows of the values in the dataframe ``df`` and explain what each column of data means. \n",
    "* identify the input and target variables\n",
    "* what is the range of values in each column? Do you need to scale, shift or normalize your data? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1634445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            X          Y  Area_Command  NIBRSOffenseCode  DayOfWeek  Time  \\\n",
      "0 -115.087518  36.216702             0                 0          0     0   \n",
      "1 -115.240172  36.189693             1                 1          1     1   \n",
      "2 -115.143088  36.181329             2                 1          2     0   \n",
      "3 -115.225014  36.117633             3                 1          1     2   \n",
      "4 -115.176708  36.095967             4                 1          1     3   \n",
      "\n",
      "   VictimCount  \n",
      "0            0  \n",
      "1            0  \n",
      "2            1  \n",
      "3            2  \n",
      "4            0  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a8b3bd",
   "metadata": {},
   "source": [
    "**X**: coordinates of the incident location\n",
    "**Y**: coordinates of the incident location\n",
    "**Area_Command**: The command area where the incident occurred\n",
    "**DayOfWeek**: Day of the week on which the incident reported\n",
    "**Time**: Time when the incident was reported\n",
    "**NIBRSOffenseCode**: Type of criminal offense\n",
    "**VictimCount**: Number of people involved in the incident\n",
    "\n",
    "Area_Command, DayOfWeek, Time, NIBRSOffenseCode, VictimCount are factorized such that the data in these columns now represent the data in numbers.\n",
    "\n",
    "`inputs` are *X*, *Y*, *Area_Command*, *DayOfWeek*, *Time*, *NIBRSOffenseCode*.\n",
    "`Target variable` is *VictimCount*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89c0970",
   "metadata": {},
   "source": [
    "These columns need no scaling,shifting or normalizing the data as they are preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549ecc9-3c0b-4efa-9a1f-340a25a1e4be",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "* Create two `DataLoader` objects for training and testing based on the input and output variables. Pick a reasonable batch size and verify the shape of data by iterating over the one dataset and printing the shape of the batched data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00fe4287-934b-4799-9e43-c3571acfbab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([64, 6])\n",
      "Batch Y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X = df[['X', 'Y', 'Area_Command', 'NIBRSOffenseCode', 'DayOfWeek', 'Time']]\n",
    "Y = df['VictimCount']\n",
    "\n",
    "# Convert the Pandas DataFrames to PyTorch tensors\n",
    "X = torch.tensor(X.values, dtype=torch.float32)\n",
    "Y = torch.tensor(Y.values, dtype=torch.float32)\n",
    "\n",
    "# Define a reasonable batch size (e.g., 64)\n",
    "batch_size = 64\n",
    "\n",
    "# Create a TensorDataset for the data\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "# Split the dataset into training and testing sets (e.g., 80% for training, 20% for testing)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verify the shape of data by iterating over one dataset and printing the shape of the batched data (e.g., training data)\n",
    "for batch_X, batch_Y in train_loader:\n",
    "    print(\"Batch X shape:\", batch_X.shape)\n",
    "    print(\"Batch Y shape:\", batch_Y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6f08c-5e70-4b14-b62c-4686d9f7aace",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task you will try to predict number of crime victims as a **real number**. Therefore the machine learning problem is a **regression** problem. \n",
    "\n",
    "* Define the proper loss function for this task\n",
    "* what should the size of the predicted output be?\n",
    "* explain your choice of architecture, including how many layers you will be using\n",
    "* define an optimizer for training this model, choose a proper learning rate \n",
    "* write a training loop that obtains a batch out of the  training data and calculates the forward and backward passes over the neural network. Call the optimizer to update the weights of the neural network.\n",
    "* write a for loop that continues the training over a number of epochs. At the end of each epoch, calculate the ``MSE`` error on the test data and print it.\n",
    "* is your model training well? Adjust the learning rate, hidden size of the network, and try different activation functions and number of layers to achieve the best accuracy and report it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e10d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\030742879\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\030742879\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([28])) that is different to the input size (torch.Size([28, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\030742879\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([55])) that is different to the input size (torch.Size([55, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 92.3593, Test Loss: 59.0809\n",
      "Epoch [2/50] - Train Loss: 43.2761, Test Loss: 25.6219\n",
      "Epoch [3/50] - Train Loss: 17.9257, Test Loss: 9.9136\n",
      "Epoch [4/50] - Train Loss: 6.3206, Test Loss: 3.3751\n",
      "Epoch [5/50] - Train Loss: 1.9162, Test Loss: 1.3641\n",
      "Epoch [6/50] - Train Loss: 0.9327, Test Loss: 1.2403\n",
      "Epoch [7/50] - Train Loss: 1.1911, Test Loss: 1.6000\n",
      "Epoch [8/50] - Train Loss: 1.6063, Test Loss: 1.8562\n",
      "Epoch [9/50] - Train Loss: 1.7233, Test Loss: 1.8466\n",
      "Epoch [10/50] - Train Loss: 1.6523, Test Loss: 1.6649\n",
      "Epoch [11/50] - Train Loss: 1.4224, Test Loss: 1.4401\n",
      "Epoch [12/50] - Train Loss: 1.1595, Test Loss: 1.2769\n",
      "Epoch [13/50] - Train Loss: 1.0144, Test Loss: 1.1942\n",
      "Epoch [14/50] - Train Loss: 1.0301, Test Loss: 1.1851\n",
      "Epoch [15/50] - Train Loss: 0.8910, Test Loss: 1.2097\n",
      "Epoch [16/50] - Train Loss: 1.0050, Test Loss: 1.2380\n",
      "Epoch [17/50] - Train Loss: 0.9682, Test Loss: 1.2429\n",
      "Epoch [18/50] - Train Loss: 1.0048, Test Loss: 1.2296\n",
      "Epoch [19/50] - Train Loss: 0.9019, Test Loss: 1.2119\n",
      "Epoch [20/50] - Train Loss: 0.9314, Test Loss: 1.1971\n",
      "Epoch [21/50] - Train Loss: 0.9471, Test Loss: 1.1882\n",
      "Epoch [22/50] - Train Loss: 0.8707, Test Loss: 1.1833\n",
      "Epoch [23/50] - Train Loss: 0.8610, Test Loss: 1.1823\n",
      "Epoch [24/50] - Train Loss: 0.8840, Test Loss: 1.1829\n",
      "Epoch [25/50] - Train Loss: 0.8937, Test Loss: 1.1854\n",
      "Epoch [26/50] - Train Loss: 1.0392, Test Loss: 1.1878\n",
      "Epoch [27/50] - Train Loss: 0.8952, Test Loss: 1.1888\n",
      "Epoch [28/50] - Train Loss: 0.9088, Test Loss: 1.1890\n",
      "Epoch [29/50] - Train Loss: 0.9474, Test Loss: 1.1897\n",
      "Epoch [30/50] - Train Loss: 0.8816, Test Loss: 1.1880\n",
      "Epoch [31/50] - Train Loss: 0.8916, Test Loss: 1.1881\n",
      "Epoch [32/50] - Train Loss: 0.8328, Test Loss: 1.1878\n",
      "Epoch [33/50] - Train Loss: 0.8938, Test Loss: 1.1872\n",
      "Epoch [34/50] - Train Loss: 0.8710, Test Loss: 1.1864\n",
      "Epoch [35/50] - Train Loss: 1.0104, Test Loss: 1.1874\n",
      "Epoch [36/50] - Train Loss: 0.9557, Test Loss: 1.1877\n",
      "Epoch [37/50] - Train Loss: 0.9292, Test Loss: 1.1842\n",
      "Epoch [38/50] - Train Loss: 1.1072, Test Loss: 1.1831\n",
      "Epoch [39/50] - Train Loss: 1.0308, Test Loss: 1.1808\n",
      "Epoch [40/50] - Train Loss: 0.9780, Test Loss: 1.1791\n",
      "Epoch [41/50] - Train Loss: 0.8723, Test Loss: 1.1788\n",
      "Epoch [42/50] - Train Loss: 0.8715, Test Loss: 1.1803\n",
      "Epoch [43/50] - Train Loss: 0.8718, Test Loss: 1.1840\n",
      "Epoch [44/50] - Train Loss: 0.8500, Test Loss: 1.1891\n",
      "Epoch [45/50] - Train Loss: 0.8852, Test Loss: 1.1919\n",
      "Epoch [46/50] - Train Loss: 0.9189, Test Loss: 1.1890\n",
      "Epoch [47/50] - Train Loss: 1.0551, Test Loss: 1.1847\n",
      "Epoch [48/50] - Train Loss: 0.8319, Test Loss: 1.1807\n",
      "Epoch [49/50] - Train Loss: 0.8649, Test Loss: 1.1795\n",
      "Epoch [50/50] - Train Loss: 0.9098, Test Loss: 1.1795\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the neural network architecture\n",
    "class CustomCrimePredictionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CustomCrimePredictionModel, self).__init__()  # Correct the typo\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjust the hidden size as needed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)  # Adjust the hidden size as needed\n",
    "        self.fc3 = nn.Linear(32, 1)  # Output size is 1 for regression\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# Define the input size based on your dataset\n",
    "input_size = 6  # Adjust as needed based on the number of features in your dataset\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CustomCrimePredictionModel(input_size)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "        loss = loss_function(predictions, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            predictions = model(batch_X)\n",
    "            loss = loss_function(predictions, batch_Y)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3fc70-c6ce-4589-9930-128951290e8d",
   "metadata": {},
   "source": [
    "## Task 4 \n",
    "\n",
    "In this task, you will try to predict the number of crime victims as a **class number**. Therefore the machine learning problem is a **classification** problem. \n",
    "\n",
    "* Repeat all the steps in task 3. Specifically, pay attention to the differences with regression.\n",
    "* How would you find the number of classes on the output data?\n",
    "* How is the architecture different?\n",
    "* How is the loss function different?\n",
    "* Calculate the Accuracy for test data as the number of correct classified outputs divided by the total number of test data in each epoch. Report it at the end of each epoch\n",
    "* Try a few variations of learning rate, hidden dimensions, layers, etc. What is the best accuracy that you can get? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339cfe68",
   "metadata": {},
   "source": [
    "Number of Classes:\n",
    "To determine the number of classes for the classification problem, you can look at the unique values in the 'VictimCount' column in your dataset. In your given data, you mentioned that 'VictimCount' has a maximum value of 15. This means you have 16 different classes (0 to 15) to predict.\n",
    "\n",
    "Architecture Differences:\n",
    "For a classification task, you should use a neural network architecture designed for classification. You can use a model with a final layer that has as many output units as there are classes (16 in this case) and use an appropriate activation function like Softmax to convert the network's output into class probabilities.\n",
    "\n",
    "Loss Function Differences:\n",
    "In classification problems, you typically use a different loss function, such as Cross-Entropy (also known as Negative Log-Likelihood) loss. This loss function measures the dissimilarity between the predicted class probabilities and the true class labels.\n",
    "\n",
    "Accuracy Calculation:\n",
    "To calculate accuracy on the test data, you need to compare the model's predicted class labels to the true class labels and calculate the proportion of correct predictions. This can be done at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d929c52-af34-4081-92cd-3463a3fc4db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] - Train Loss: 4.6297, Train Accuracy: 0.0045, Test Loss: 3.4678, Test Accuracy: 0.0000\n",
      "Epoch [2/50] - Train Loss: 2.9761, Train Accuracy: 0.0045, Test Loss: 2.4559, Test Accuracy: 0.0000\n",
      "Epoch [3/50] - Train Loss: 2.1069, Train Accuracy: 0.2409, Test Loss: 1.9846, Test Accuracy: 0.5091\n",
      "Epoch [4/50] - Train Loss: 1.7448, Train Accuracy: 0.4909, Test Loss: 1.8143, Test Accuracy: 0.4727\n",
      "Epoch [5/50] - Train Loss: 1.6918, Train Accuracy: 0.4318, Test Loss: 1.6707, Test Accuracy: 0.5273\n",
      "Epoch [6/50] - Train Loss: 1.5265, Train Accuracy: 0.5136, Test Loss: 1.5669, Test Accuracy: 0.5273\n",
      "Epoch [7/50] - Train Loss: 1.3945, Train Accuracy: 0.5136, Test Loss: 1.4956, Test Accuracy: 0.5273\n",
      "Epoch [8/50] - Train Loss: 1.3056, Train Accuracy: 0.5136, Test Loss: 1.4682, Test Accuracy: 0.4545\n",
      "Epoch [9/50] - Train Loss: 1.3670, Train Accuracy: 0.4273, Test Loss: 1.4414, Test Accuracy: 0.5091\n",
      "Epoch [10/50] - Train Loss: 1.3357, Train Accuracy: 0.4591, Test Loss: 1.4139, Test Accuracy: 0.5273\n",
      "Epoch [11/50] - Train Loss: 1.2685, Train Accuracy: 0.5136, Test Loss: 1.3910, Test Accuracy: 0.5273\n",
      "Epoch [12/50] - Train Loss: 1.2513, Train Accuracy: 0.5136, Test Loss: 1.3712, Test Accuracy: 0.5273\n",
      "Epoch [13/50] - Train Loss: 1.3111, Train Accuracy: 0.5136, Test Loss: 1.3595, Test Accuracy: 0.5273\n",
      "Epoch [14/50] - Train Loss: 1.2164, Train Accuracy: 0.5136, Test Loss: 1.3373, Test Accuracy: 0.5273\n",
      "Epoch [15/50] - Train Loss: 1.1607, Train Accuracy: 0.5136, Test Loss: 1.3244, Test Accuracy: 0.5273\n",
      "Epoch [16/50] - Train Loss: 1.2529, Train Accuracy: 0.5136, Test Loss: 1.3184, Test Accuracy: 0.5091\n",
      "Epoch [17/50] - Train Loss: 1.1714, Train Accuracy: 0.5182, Test Loss: 1.3037, Test Accuracy: 0.5455\n",
      "Epoch [18/50] - Train Loss: 1.1549, Train Accuracy: 0.5045, Test Loss: 1.2828, Test Accuracy: 0.5273\n",
      "Epoch [19/50] - Train Loss: 1.1268, Train Accuracy: 0.5136, Test Loss: 1.2740, Test Accuracy: 0.5273\n",
      "Epoch [20/50] - Train Loss: 1.1416, Train Accuracy: 0.5136, Test Loss: 1.2671, Test Accuracy: 0.5273\n",
      "Epoch [21/50] - Train Loss: 1.1520, Train Accuracy: 0.5091, Test Loss: 1.2639, Test Accuracy: 0.5273\n",
      "Epoch [22/50] - Train Loss: 1.1100, Train Accuracy: 0.5136, Test Loss: 1.2487, Test Accuracy: 0.5273\n",
      "Epoch [23/50] - Train Loss: 1.1304, Train Accuracy: 0.5136, Test Loss: 1.2388, Test Accuracy: 0.5273\n",
      "Epoch [24/50] - Train Loss: 1.1223, Train Accuracy: 0.5136, Test Loss: 1.2341, Test Accuracy: 0.5273\n",
      "Epoch [25/50] - Train Loss: 1.0864, Train Accuracy: 0.4909, Test Loss: 1.2364, Test Accuracy: 0.5273\n",
      "Epoch [26/50] - Train Loss: 1.1228, Train Accuracy: 0.5045, Test Loss: 1.2271, Test Accuracy: 0.5273\n",
      "Epoch [27/50] - Train Loss: 1.1045, Train Accuracy: 0.5136, Test Loss: 1.2166, Test Accuracy: 0.5273\n",
      "Epoch [28/50] - Train Loss: 1.0682, Train Accuracy: 0.5136, Test Loss: 1.2122, Test Accuracy: 0.5273\n",
      "Epoch [29/50] - Train Loss: 1.0750, Train Accuracy: 0.5136, Test Loss: 1.2215, Test Accuracy: 0.5091\n",
      "Epoch [30/50] - Train Loss: 1.0937, Train Accuracy: 0.5227, Test Loss: 1.2248, Test Accuracy: 0.5273\n",
      "Epoch [31/50] - Train Loss: 1.1351, Train Accuracy: 0.5136, Test Loss: 1.2122, Test Accuracy: 0.5273\n",
      "Epoch [32/50] - Train Loss: 1.0536, Train Accuracy: 0.5136, Test Loss: 1.2100, Test Accuracy: 0.5273\n",
      "Epoch [33/50] - Train Loss: 1.0657, Train Accuracy: 0.5136, Test Loss: 1.2162, Test Accuracy: 0.5273\n",
      "Epoch [34/50] - Train Loss: 1.0715, Train Accuracy: 0.4727, Test Loss: 1.2186, Test Accuracy: 0.5273\n",
      "Epoch [35/50] - Train Loss: 1.0657, Train Accuracy: 0.5227, Test Loss: 1.2177, Test Accuracy: 0.5273\n",
      "Epoch [36/50] - Train Loss: 1.0887, Train Accuracy: 0.5136, Test Loss: 1.2185, Test Accuracy: 0.5273\n",
      "Epoch [37/50] - Train Loss: 1.0562, Train Accuracy: 0.5136, Test Loss: 1.2207, Test Accuracy: 0.5273\n",
      "Epoch [38/50] - Train Loss: 1.0715, Train Accuracy: 0.4864, Test Loss: 1.2228, Test Accuracy: 0.5273\n",
      "Epoch [39/50] - Train Loss: 1.0569, Train Accuracy: 0.5045, Test Loss: 1.2101, Test Accuracy: 0.5273\n",
      "Epoch [40/50] - Train Loss: 1.0759, Train Accuracy: 0.5136, Test Loss: 1.2138, Test Accuracy: 0.5273\n",
      "Epoch [41/50] - Train Loss: 1.0473, Train Accuracy: 0.5136, Test Loss: 1.2228, Test Accuracy: 0.5273\n",
      "Epoch [42/50] - Train Loss: 1.0266, Train Accuracy: 0.5182, Test Loss: 1.2264, Test Accuracy: 0.5273\n",
      "Epoch [43/50] - Train Loss: 1.0755, Train Accuracy: 0.5136, Test Loss: 1.2211, Test Accuracy: 0.5273\n",
      "Epoch [44/50] - Train Loss: 1.0558, Train Accuracy: 0.5136, Test Loss: 1.2131, Test Accuracy: 0.5273\n",
      "Epoch [45/50] - Train Loss: 1.0611, Train Accuracy: 0.5136, Test Loss: 1.2219, Test Accuracy: 0.5273\n",
      "Epoch [46/50] - Train Loss: 1.0556, Train Accuracy: 0.5136, Test Loss: 1.2226, Test Accuracy: 0.5273\n",
      "Epoch [47/50] - Train Loss: 1.0987, Train Accuracy: 0.5136, Test Loss: 1.2200, Test Accuracy: 0.5273\n",
      "Epoch [48/50] - Train Loss: 1.0451, Train Accuracy: 0.4682, Test Loss: 1.2272, Test Accuracy: 0.5273\n",
      "Epoch [49/50] - Train Loss: 1.0626, Train Accuracy: 0.4955, Test Loss: 1.2154, Test Accuracy: 0.5273\n",
      "Epoch [50/50] - Train Loss: 1.0400, Train Accuracy: 0.5136, Test Loss: 1.2149, Test Accuracy: 0.5273\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the neural network architecture for classification\n",
    "class CrimeClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CrimeClassificationModel, self).__init__()  # Correct the typo\n",
    "        self.fc1 = nn.Linear(input_size, 64)  # Adjust the hidden size as needed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)  # Adjust the hidden size as needed\n",
    "        self.fc3 = nn.Linear(32, num_classes)  # Output size is the number of classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the loss function for classification\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the input size based on your dataset\n",
    "input_size = 6  # Adjust as needed based on the number of features in your dataset\n",
    "num_classes = 16  # The number of classes\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CrimeClassificationModel(input_size, num_classes)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Create DataLoader objects for training and testing (as previously described)\n",
    "# Ensure you have already prepared the train_loader and test_loader\n",
    "\n",
    "# Training loop with accuracy calculation (as previously described)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0  # Counter for correct training predictions\n",
    "    \n",
    "    for batch_X, batch_Y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_X)\n",
    "\n",
    "        # Ensure the labels are of type Long\n",
    "        batch_Y = batch_Y.to(torch.long)\n",
    "\n",
    "        loss = loss_function(predictions, batch_Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Ensure the model's output is of type Long\n",
    "        _, predicted = torch.max(predictions, 1)\n",
    "        correct_train += (predicted == batch_Y).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = correct_train / len(train_loader.dataset)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0  # Counter for correct test predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_Y in test_loader:\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Ensure the labels are of type Long\n",
    "            batch_Y = batch_Y.to(torch.long)\n",
    "\n",
    "            loss = loss_function(predictions, batch_Y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Ensure the model's output is of type Long\n",
    "            _, predicted = torch.max(predictions, 1)\n",
    "            correct_test += (predicted == batch_Y).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = correct_test / len(test_loader.dataset)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d2a304-6197-4cd9-b31e-745f7862f213",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4ef16-d828-45e5-bd58-1c49fcfecf52",
   "metadata": {},
   "source": [
    "### Reflect on your results\n",
    "\n",
    "* Write a paragraph about your experience with tasks 3 and 4. How do you compare the results? Which one worked better? Why?\n",
    "* Write a piece of code that finds an example of a  miss-classification. Calculate the probabilities for the output classes and plot them in a bar chart. Also, indicate what is the correct class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf70fd6",
   "metadata": {},
   "source": [
    "We've addressed two distinct machine learning challenges: regression and classification, both with the objective of forecasting the number of crime victims. Regression revolves around predicting a continuous numeric outcome, while classification entails classifying data into distinct categories. I've supplied code and guidance for both tasks. The choice between these approaches hinges on the specific objectives and criteria for success.\n",
    "\n",
    "Regression is well-suited for scenarios where the aim is to predict precise numerical values, and its performance can be assessed using metrics such as Mean Squared Error (MSE). In contrast, classification is designed for tasks involving the allocation of data into predefined classes, with accuracy and other classification metrics serving as common measures of performance. The selection between regression and classification hinges on the nature of the problem and the analytical objectives. If the primary goal is to forecast the exact count of crime victims, regression is typically the more appropriate choice. However, if the objective is to categorize crime incidents into severity classes (e.g., low, medium, high), classification becomes the preferred approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9dc3a3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4194030069.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\030742879\\AppData\\Local\\Temp\\ipykernel_18720\\4194030069.py\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    plt.bar(class_labels, probabilities, tick_label=class_labels)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "misclassified_examples = []\n",
    "true_labels = []\n",
    "\n",
    "# Loop through the test data\n",
    "for batch_X, batch_Y in test_loader:\n",
    "    predictions = model(batch_X)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    # Find misclassified examples\n",
    "    misclassified_indices = (predicted_labels != batch_Y).nonzero()\n",
    "    \n",
    "    if misclassified_indices.numel() > 0:\n",
    "        misclassified_examples.extend(batch_X[misclassified_indices])\n",
    "        true_labels.extend(batch_Y[misclassified_indices])\n",
    "        break  # Stop after finding the first misclassified example\n",
    "\n",
    "# Display the first misclassified example and its true label\n",
    "if len(misclassified_examples) > 0:\n",
    "    misclassified_example = misclassified_examples[0]\n",
    "    true_label = true_labels[0]\n",
    "\n",
    "    print(f'Misclassified Example True Label: {true_label.item()}')\n",
    "    \n",
    "    # Calculate class probabilities using softmax\n",
    "    probabilities = torch.softmax(predictions[0], dim=0).tolist()\n",
    "    class_labels = list(range(num_classes)\n",
    "\n",
    "    # Plot the class probabilities\n",
    "    plt.bar(class_labels, probabilities, tick_label=class_labels)\n",
    "    plt.xlabel('Class Labels')\n",
    "    plt.ylabel('Class Probabilities')\n",
    "    plt.title('Class Probabilities for Misclassified Example')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No misclassified examples found in the test data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b000b0b-fa37-4f8d-8ec7-0a1e6e749424",
   "metadata": {},
   "source": [
    "## Task 6: Exploring the patterns in raw data\n",
    "\n",
    "* Plot the crime incidents as a `scatter` plot using the corrdinates. Use the color property of each datapoint to indicate the day of the week. Is there a pattern in the plot?\n",
    "* Now make a new scatter plot and use the color property of each datapoint to indicate the number of persons involved in the incident. Is there a pattern here?\n",
    "* use numpy (or pandas if you like) to sort the number of crimes reported by the day of the week. What days are most frequent?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b396aaf-5d1a-49cd-ae1d-b63af7e561f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
